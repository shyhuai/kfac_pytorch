diff --git a/examples/pytorch_cifar10_resnet.py b/examples/pytorch_cifar10_resnet.py
index 6a1d148..f41e8ae 100644
--- a/examples/pytorch_cifar10_resnet.py
+++ b/examples/pytorch_cifar10_resnet.py
@@ -8,7 +8,6 @@ import math
 from distutils.version import LooseVersion
 
 import torch
-torch.multiprocessing.set_start_method('spawn')
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
@@ -27,6 +26,10 @@ import kfac
 import logging
 import horovod.torch as hvd
 
+import wandb
+
+#torch.multiprocessing.set_start_method('spawn')
+
 logger = logging.getLogger()
 logger.setLevel(logging.INFO)
 strhdlr = logging.StreamHandler()
@@ -64,6 +67,10 @@ parser.add_argument('--weight-decay', type=float, default=5e-4, metavar='W',
                     help='SGD weight decay (default: 5e-4)')
 
 # KFAC Parameters
+parser.add_argument('--kfac-name', type=str, default='inverse',
+                    help='choises: %s' % kfac.kfac_mappers.keys() + ', default: '+'inverse')
+parser.add_argument('--sparse-ratio', type=float, default=1,
+                    help='Specify the sparse ratio if kfac-name is inverse_sparse')
 parser.add_argument('--kfac-update-freq', type=int, default=10,
                     help='iters between kfac inv ops (0 for no kfac updates) (default: 10)')
 parser.add_argument('--kfac-cov-update-freq', type=int, default=1,
@@ -107,7 +114,13 @@ args.cuda = not args.no_cuda and torch.cuda.is_available()
 # Horovod: initialize library.
 hvd.init()
 
-logfile = './logs/sparse_cifar10_{}_kfac{}_gpu{}_bs{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size)
+
+logfilename = 'convergence_cifar10_{}_kfac{}_gpu{}_bs{}_{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size, args.kfac_name)
+if hvd.rank() == 0:
+    wandb.init(project='kfac', entity='shyhuai', name=logfilename, config=args)
+
+logfile = './logs/'+logfilename
+#logfile = './logs/sparse_cifar10_{}_kfac{}_gpu{}_bs{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size)
 #logfile = './logs/cifar10_{}_kfac{}_gpu{}_bs{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size)
 hdlr = logging.FileHandler(logfile)
 hdlr.setFormatter(formatter)
@@ -195,13 +208,15 @@ optimizer = optim.SGD(model.parameters(), lr=args.base_lr, momentum=args.momentu
                       weight_decay=args.weight_decay)
 
 if use_kfac:
-    preconditioner = kfac.KFAC(model, lr=args.base_lr, factor_decay=args.stat_decay, 
+    KFAC = kfac.get_kfac_module(args.kfac_name)
+    preconditioner = KFAC(model, lr=args.base_lr, factor_decay=args.stat_decay, 
                                damping=args.damping, kl_clip=args.kl_clip, 
                                fac_update_freq=args.kfac_cov_update_freq, 
                                kfac_update_freq=args.kfac_update_freq,
                                diag_blocks=args.diag_blocks,
                                diag_warmup=args.diag_warmup,
-                               distribute_layer_factors=args.distribute_layer_factors)
+                               distribute_layer_factors=args.distribute_layer_factors,
+                               sparse_ratio=args.sparse_ratio)
     kfac_param_scheduler = kfac.KFACParamScheduler(preconditioner,
             damping_alpha=args.damping_alpha,
             damping_schedule=args.damping_schedule,
@@ -275,6 +290,8 @@ def train(epoch):
                 if args.verbose:
                     logger.info("[%d][%d] train loss: %.4f, acc: %.3f, time: %.3f, speed: %.3f images/s" % (epoch, batch_idx, train_loss.avg.item(), 100*train_accuracy.avg.item(), avg_time/display, args.batch_size/(avg_time/display)))
                     avg_time = 0.0
+            if hvd.rank() == 0:
+                wandb.log({"loss": loss})
         if args.verbose:
             logger.info("[%d] epoch train loss: %.4f, acc: %.3f" % (epoch, train_loss.avg.item(), 100*train_accuracy.avg.item()))
 
@@ -307,6 +324,8 @@ def test(epoch):
                 test_accuracy.update(accuracy(output, target))
             if args.verbose:
                 logger.info("[%d][0] evaluation loss: %.4f, acc: %.3f" % (epoch, test_loss.avg.item(), 100*test_accuracy.avg.item()))
+                if hvd.rank() == 0:
+                    wandb.log({"val top-1 acc": test_accuracy.avg.item()})
                 
                 #t.update(1)
                 #if i + 1 == len(test_loader):
diff --git a/examples/pytorch_imagenet_resnet.py b/examples/pytorch_imagenet_resnet.py
index 5b492b8..88bf4cf 100644
--- a/examples/pytorch_imagenet_resnet.py
+++ b/examples/pytorch_imagenet_resnet.py
@@ -172,7 +172,7 @@ def initialize():
     except ImportError:
         args.log_writer = None
 
-    logfile = './logs/debug_timing_dynamicmerge_ns1_imagenet_thres1024_{}_kfac{}_gpu{}_bs{}_{}_ep_{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size, args.kfac_name, args.exclude_parts)
+    logfile = './logs/convergence_timing_dynamicmerge_ns1_imagenet_thres1024_{}_kfac{}_gpu{}_bs{}_{}_ep_{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size, args.kfac_name, args.exclude_parts)
     #logfile = './logs/timing_notf_imagenet_thres1024_{}_kfac{}_gpu{}_bs{}_{}_ep_{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size, args.kfac_name, args.exclude_parts)
     #logfile = './logs/timing_ttf_imagenet_thres1024_{}_kfac{}_gpu{}_bs{}_{}_ep_{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size, args.kfac_name, args.exclude_parts)
     #logfile = './logs/timing_imagenet_{}_kfac{}_gpu{}_bs{}_{}_ep_{}.log'.format(args.model, args.kfac_update_freq, hvd.size(), args.batch_size, args.kfac_name, args.exclude_parts)
@@ -435,7 +435,7 @@ if __name__ == '__main__':
     for epoch in range(args.resume_from_epoch, args.epochs):
         train(epoch, model, opt, preconditioner, lr_schedules, lrs,
              loss_func, train_sampler, train_loader, args)
-        #validate(epoch, model, loss_func, val_loader, args)
+        validate(epoch, model, loss_func, val_loader, args)
         #save_checkpoint(model, opt, args.checkpoint_format, epoch)
 
     #if args.verbose:
diff --git a/horovod_mpi_cj.sh b/horovod_mpi_cj.sh
index 298ea4b..69776ca 100755
--- a/horovod_mpi_cj.sh
+++ b/horovod_mpi_cj.sh
@@ -5,6 +5,7 @@ nworkers="${nworkers:-4}"
 batch_size="${batch_size:-32}"
 rdma="${rdma:-1}"
 kfac="${kfac:-1}"
+lr="${lr:-0.1}"
 epochs="${epochs:-55}"
 kfac_name="${kfac_name:-inverse}"
 exclude_parts="${exclude_parts:-''}"
@@ -35,14 +36,15 @@ if [ "$dnn" = "resnet32" ]; then
 $MPIPATH/bin/mpirun --oversubscribe --prefix $MPIPATH -np $nworkers -hostfile cluster${nworkers} -bind-to none -map-by slot \
     $params \
     $PY examples/pytorch_cifar10_resnet.py \
-        --base-lr 0.1 --epochs 100 --kfac-update-freq $kfac --model $dnn --lr-decay 35 75 90 --batch-size $batch_size 
+        --base-lr $lr --epochs 100 --kfac-update-freq $kfac --model $dnn --lr-decay 35 75 90 --batch-size $batch_size 
 else
 #HOROVOD_TIMELINE=./logs/profile-timeline-${dnn}-kfac-${kfac}-json.log 
 $MPIPATH/bin/mpirun --oversubscribe --prefix $MPIPATH -np $nworkers -hostfile cluster${nworkers} -bind-to none -map-by slot \
     $params \
     $PY examples/pytorch_imagenet_resnet.py \
-          --base-lr 0.0125 --epochs $epochs --kfac-update-freq $kfac --kfac-cov-update-freq $kfac --model $dnn --kfac-name $kfac_name --exclude-parts ${exclude_parts} --batch-size $batch_size --lr-decay 25 35 40 45 50 \
+          --base-lr $lr --epochs $epochs --kfac-update-freq $kfac --kfac-cov-update-freq $kfac --model $dnn --kfac-name $kfac_name --exclude-parts ${exclude_parts} --batch-size $batch_size --lr-decay 10 15 20 28 30 \
           --train-dir /localdata/ILSVRC2012_dataset/train \
           --val-dir /localdata/ILSVRC2012_dataset/val
           #--base-lr 0.0125 --epochs 20 --kfac-update-freq $kfac --kfac-cov-update-freq $kfac --model $dnn  --batch-size $batch_size --lr-decay 8 14 16 18 --damping 0.0015 \
+          #--base-lr 0.0125 --epochs $epochs --kfac-update-freq $kfac --kfac-cov-update-freq $kfac --model $dnn --kfac-name $kfac_name --exclude-parts ${exclude_parts} --batch-size $batch_size --lr-decay 25 35 40 45 50 \ 
 fi
diff --git a/icdcs2021-logs/factorcompute-fp16-resnet50.oog b/icdcs2021-logs/factorcompute-fp16-resnet50.oog
new file mode 100644
index 0000000..8c711f0
--- /dev/null
+++ b/icdcs2021-logs/factorcompute-fp16-resnet50.oog
@@ -0,0 +1,114 @@
+[(100352, 147), (25088, 64), (25088, 576), (25088, 64), (25088, 64), (25088, 256), (25088, 576), (25088, 64), (25088, 256), (25088, 576), (25088, 64), (25088, 256), (6272, 1152), (6272, 128), (6272, 256), (6272, 512), (6272, 1152), (6272, 128), (6272, 512), (6272, 1152), (6272, 128), (6272, 512), (6272, 1152), (6272, 128), (6272, 512), (1568, 2304), (1568, 256), (1568, 512), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (392, 4608), (392, 512), (392, 1024), (392, 2048), (392, 4608), (392, 512), (392, 2048), (392, 4608), (392, 512), (8, 2049), (100352, 64), (25088, 64), (25088, 64), (25088, 256), (25088, 256), (25088, 64), (25088, 64), (25088, 256), (25088, 64), (25088, 64), (25088, 256), (25088, 128), (6272, 128), (6272, 512), (6272, 512), (6272, 128), (6272, 128), (6272, 512), (6272, 128), (6272, 128), (6272, 512), (6272, 128), (6272, 128), (6272, 512), (6272, 256), (1568, 256), (1568, 1024), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 512), (392, 512), (392, 2048), (392, 2048), (392, 512), (392, 512), (392, 2048), (392, 512), (392, 512), (392, 2048), (8, 1000)]
+(401408,147),0.014760
+(100352,64),0.000929
+(100352,576),0.015337
+(100352,64),0.000928
+(100352,64),0.000928
+(100352,256),0.007154
+(100352,576),0.015333
+(100352,64),0.000929
+(100352,256),0.007265
+(100352,576),0.015333
+(100352,64),0.000929
+(100352,256),0.007158
+(25088,1152),0.003106
+(25088,128),0.000196
+(25088,256),0.000349
+(25088,512),0.000767
+(25088,1152),0.003102
+(25088,128),0.000197
+(25088,512),0.000765
+(25088,1152),0.003105
+(25088,128),0.000196
+(25088,512),0.000767
+(25088,1152),0.003104
+(25088,128),0.000197
+(25088,512),0.000763
+(6272,2304),0.001846
+(6272,256),0.000137
+(6272,512),0.000219
+(6272,1024),0.000471
+(6272,2304),0.001853
+(6272,256),0.000138
+(6272,1024),0.000471
+(6272,2304),0.001837
+(6272,256),0.000140
+(6272,1024),0.000471
+(6272,2304),0.001848
+(6272,256),0.000138
+(6272,1024),0.000472
+(6272,2304),0.001841
+(6272,256),0.000140
+(6272,1024),0.000470
+(6272,2304),0.001849
+(6272,256),0.000139
+(6272,1024),0.000471
+(1568,4608),0.001660
+(1568,512),0.000089
+(1568,1024),0.000149
+(1568,2048),0.000408
+(1568,4608),0.001665
+(1568,512),0.000088
+(1568,2048),0.000414
+(1568,4608),0.001660
+(1568,512),0.000089
+(32,2049),0.000121
+(401408,64),0.003940
+(100352,64),0.000929
+(100352,64),0.000928
+(100352,256),0.007185
+(100352,256),0.007157
+(100352,64),0.000928
+(100352,64),0.000928
+(100352,256),0.007150
+(100352,64),0.000928
+(100352,64),0.000929
+(100352,256),0.007183
+(100352,128),0.002102
+(25088,128),0.000197
+(25088,512),0.000768
+(25088,512),0.000768
+(25088,128),0.000199
+(25088,128),0.000196
+(25088,512),0.000767
+(25088,128),0.000198
+(25088,128),0.000197
+(25088,512),0.000766
+(25088,128),0.000198
+(25088,128),0.000197
+(25088,512),0.000765
+(25088,256),0.000349
+(6272,256),0.000133
+(6272,1024),0.000470
+(6272,1024),0.000471
+(6272,256),0.000134
+(6272,256),0.000133
+(6272,1024),0.000470
+(6272,256),0.000137
+(6272,256),0.000134
+(6272,1024),0.000470
+(6272,256),0.000135
+(6272,256),0.000134
+(6272,1024),0.000470
+(6272,256),0.000133
+(6272,256),0.000133
+(6272,1024),0.000470
+(6272,256),0.000133
+(6272,256),0.000134
+(6272,1024),0.000473
+(6272,512),0.000220
+(1568,512),0.000086
+(1568,2048),0.000410
+(1568,2048),0.000412
+(1568,512),0.000086
+(1568,512),0.000087
+(1568,2048),0.000409
+(1568,512),0.000087
+(1568,512),0.000087
+(1568,2048),0.000410
+(32,1000),0.000059
+Log file:  ./logs/resnet50-matrixsize-ag.log
+# of Tensors:  108
+Total size:  1053793568
+Total time:  0.17786226749420164
+Max-min-mean-std:  0.015337185859680176 5.855083465576172e-05 0.0016468728471685336 0.0031888215312578048
diff --git a/icdcs2021-logs/factorcompute-fp32-resnet50.oog b/icdcs2021-logs/factorcompute-fp32-resnet50.oog
new file mode 100644
index 0000000..07d8c46
--- /dev/null
+++ b/icdcs2021-logs/factorcompute-fp32-resnet50.oog
@@ -0,0 +1,114 @@
+[(100352, 147), (25088, 64), (25088, 576), (25088, 64), (25088, 64), (25088, 256), (25088, 576), (25088, 64), (25088, 256), (25088, 576), (25088, 64), (25088, 256), (6272, 1152), (6272, 128), (6272, 256), (6272, 512), (6272, 1152), (6272, 128), (6272, 512), (6272, 1152), (6272, 128), (6272, 512), (6272, 1152), (6272, 128), (6272, 512), (1568, 2304), (1568, 256), (1568, 512), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (1568, 2304), (1568, 256), (1568, 1024), (392, 4608), (392, 512), (392, 1024), (392, 2048), (392, 4608), (392, 512), (392, 2048), (392, 4608), (392, 512), (8, 2049), (100352, 64), (25088, 64), (25088, 64), (25088, 256), (25088, 256), (25088, 64), (25088, 64), (25088, 256), (25088, 64), (25088, 64), (25088, 256), (25088, 128), (6272, 128), (6272, 512), (6272, 512), (6272, 128), (6272, 128), (6272, 512), (6272, 128), (6272, 128), (6272, 512), (6272, 128), (6272, 128), (6272, 512), (6272, 256), (1568, 256), (1568, 1024), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 256), (1568, 256), (1568, 1024), (1568, 512), (392, 512), (392, 2048), (392, 2048), (392, 512), (392, 512), (392, 2048), (392, 512), (392, 512), (392, 2048), (8, 1000)]
+(401408,147),0.003530
+(100352,64),0.000200
+(100352,576),0.005807
+(100352,64),0.000167
+(100352,64),0.000160
+(100352,256),0.001230
+(100352,576),0.005780
+(100352,64),0.000166
+(100352,256),0.001227
+(100352,576),0.005814
+(100352,64),0.000164
+(100352,256),0.001223
+(25088,1152),0.005837
+(25088,128),0.000106
+(25088,256),0.000304
+(25088,512),0.001021
+(25088,1152),0.005829
+(25088,128),0.000112
+(25088,512),0.001039
+(25088,1152),0.005819
+(25088,128),0.000111
+(25088,512),0.001054
+(25088,1152),0.005857
+(25088,128),0.000108
+(25088,512),0.001051
+(6272,2304),0.005151
+(6272,256),0.000089
+(6272,512),0.000268
+(6272,1024),0.000956
+(6272,2304),0.005145
+(6272,256),0.000089
+(6272,1024),0.000952
+(6272,2304),0.005157
+(6272,256),0.000088
+(6272,1024),0.000942
+(6272,2304),0.005135
+(6272,256),0.000088
+(6272,1024),0.000950
+(6272,2304),0.005163
+(6272,256),0.000087
+(6272,1024),0.000951
+(6272,2304),0.005145
+(6272,256),0.000089
+(6272,1024),0.000950
+(1568,4608),0.005332
+(1568,512),0.000082
+(1568,1024),0.000279
+(1568,2048),0.000954
+(1568,4608),0.005358
+(1568,512),0.000083
+(1568,2048),0.000993
+(1568,4608),0.005353
+(1568,512),0.000081
+(32,2049),0.000054
+(401408,64),0.000497
+(100352,64),0.000160
+(100352,64),0.000160
+(100352,256),0.001235
+(100352,256),0.001229
+(100352,64),0.000164
+(100352,64),0.000160
+(100352,256),0.001233
+(100352,64),0.000165
+(100352,64),0.000160
+(100352,256),0.001226
+(100352,128),0.000331
+(25088,128),0.000102
+(25088,512),0.001027
+(25088,512),0.001081
+(25088,128),0.000115
+(25088,128),0.000106
+(25088,512),0.001045
+(25088,128),0.000113
+(25088,128),0.000105
+(25088,512),0.001057
+(25088,128),0.000113
+(25088,128),0.000106
+(25088,512),0.001069
+(25088,256),0.000321
+(6272,256),0.000080
+(6272,1024),0.000986
+(6272,1024),0.000956
+(6272,256),0.000090
+(6272,256),0.000083
+(6272,1024),0.000937
+(6272,256),0.000092
+(6272,256),0.000091
+(6272,1024),0.000962
+(6272,256),0.000087
+(6272,256),0.000087
+(6272,1024),0.000970
+(6272,256),0.000092
+(6272,256),0.000085
+(6272,1024),0.000952
+(6272,256),0.000092
+(6272,256),0.000085
+(6272,1024),0.000946
+(6272,512),0.000286
+(1568,512),0.000072
+(1568,2048),0.000978
+(1568,2048),0.000996
+(1568,512),0.000078
+(1568,512),0.000078
+(1568,2048),0.000979
+(1568,512),0.000078
+(1568,512),0.000076
+(1568,2048),0.000962
+(32,1000),0.000015
+Log file:  ./logs/resnet50-matrixsize-ag.log
+# of Tensors:  108
+Total size:  1053793568
+Total time:  0.1350305366516113
+Max-min-mean-std:  0.0058568382263183595 1.4562606811523437e-05 0.0012502827467741786 0.0018387171416262861
diff --git a/kfac/__init__.py b/kfac/__init__.py
index 87c8b49..6dc7180 100644
--- a/kfac/__init__.py
+++ b/kfac/__init__.py
@@ -1,6 +1,7 @@
 from kfac.kfac_preconditioner import KFACParamScheduler
 from kfac.kfac_preconditioner import KFAC as KFAC_EIGEN
 from kfac.kfac_preconditioner_inv import KFAC as KFAC_INV
+from kfac.kfac_preconditioner_inv_sparse import KFAC as KFAC_INV_SPARSE
 from kfac.kfac_preconditioner_inv_naive import KFAC as KFAC_INV_NAIVE
 from kfac.kfac_preconditioner_inv_naive_nopar import KFAC as KFAC_INV_NAIVE_NOPAR
 from kfac.kfac_preconditioner_inv_opt import KFAC as KFAC_INV_OPT
@@ -14,6 +15,7 @@ kfac_mappers = {
     'eigen_opt': KFAC_EIGEN_OPT,
     'inverse': KFAC_INV,
     'inverse_naive': KFAC_INV_NAIVE,
+    'inverse_sparse': KFAC_INV_SPARSE,
     'inverse_naive_nopar': KFAC_INV_NAIVE_NOPAR,
     'inverse_opt': KFAC_INV_OPT,
     'inverse_opt2': KFAC_INV_OPT2,
diff --git a/kfac/kfac_preconditioner_inv_sparse.py b/kfac/kfac_preconditioner_inv_sparse.py
new file mode 100644
index 0000000..78c47c9
--- /dev/null
+++ b/kfac/kfac_preconditioner_inv_sparse.py
@@ -0,0 +1,775 @@
+import math
+import torch
+import torch.optim as optim
+import horovod.torch as hvd
+import numpy as np
+from horovod.torch.mpi_ops import allgather_async
+
+from kfac.utils import (ComputeA, ComputeG)
+from kfac.utils import update_running_avg
+from kfac.utils import try_contiguous
+from kfac.utils import cycle
+from kfac.utils import get_block_boundary
+from kfac.utils import sparsification
+from kfac.comm import MergedCommAllReduce, MergedCommBcast, MultiTensorComm, barrier
+import logging
+import tcmm
+import torchsso
+
+logger = logging.getLogger()
+
+def add_value_to_diagonal(X, value):
+    return X.add_(torch.diag(X.new(X.shape[0]).fill_(value)))
+    #if torch.cuda.is_available():
+    #    indices = torch.cuda.LongTensor([[i, i] for i in range(X.shape[0])])
+    #else:
+    #    indices = torch.LongTensor([[i, i] for i in range(X.shape[0])])
+    #values = X.new_ones(X.shape[0]).mul(value)
+    #return X.index_put(tuple(indices.t()), values, accumulate=True)
+
+class KFAC(optim.Optimizer):
+    """KFAC Distributed Gradient Preconditioner
+
+    Computes the natural gradient of a model in place with a layer-wise
+    FIM approximation. Layer computations are distributed across workers
+    using Horovod.
+
+    Usage:
+      optimizer = optim.SGD(model.parameters(), ...)
+      optimizer = hvd.DistributedOptimizer(optimizer, ...)
+      preconditioner = KFAC(model, ...)
+      ... 
+      for i, (data, target) in enumerate(train_loader):
+          optimizer.zero_grad()
+          output = model(data)
+          loss = criterion(output, target)
+          loss.backward()
+          optimizer.synchronize()
+          preconditioner.step()
+          with optimizer.skip_synchronize():
+              optimizer.step()
+
+    Args:
+      model (nn): Torch model to precondition
+      lr (float, optional): learning rate (default: 0.1)
+      factor_decay (float, optional): running average coefficient for Kronecker
+          factors (default: 0.95)
+      damping (float, optional): Tikhonov damping parameter (default: 0.001)
+      kl_clip (float, optional): clipping parameter for gradient scaling
+          (default: 0.001)
+      fac_update_freq (int, optional): iterations between calculating and
+          updating the running average of the Kronecker factors (default: 10)
+      kfac_update_freq (int, optional): iterations between applying gradient
+          preconditioning (default: 100)
+      batch_averaged (bool, optional): boolean representing if the gradient
+          is alrady averaged across the batches (default: True)
+      diag_blocks (int, optional): Experimental: number of diagonal blocks to
+          approximate the Kronecker factor eigendecomposition with. 
+          `diag_blocks=1` computes the eigendecomposition of the entire factor
+          (default: 1)
+      diag_warmup (int, optional): number of epochs to wait before starting
+          the block diagonal factor approximation (default: 0)
+      distribute_layer_factors (bool, optional): if `True`, computes factors A
+          and G on different workers else computes A and G for a single layer
+          on the same worker. If `None`, determines best value based on layer
+          count (default: None)
+    """
+    def __init__(self,
+                 model,
+                 lr=0.1,
+                 factor_decay=0.95,
+                 damping=0.001,
+                 kl_clip=0.001,
+                 fac_update_freq=10,
+                 kfac_update_freq=100,
+                 batch_averaged=True,
+                 diag_blocks=1,
+                 diag_warmup=0,
+                 distribute_layer_factors=None,
+                 sparse=True,
+                 sparse_ratio=0.01,
+                 exclude_parts=''):
+                 #exclude_parts='CommunicateInverse,ComputeInverse,CommunicateFactor,ComputeFactor'):
+
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 < factor_decay <= 1:
+            raise ValueError("Invalid factor decay rate: {}".format(factor_decay))
+        if not 0.0 < damping:
+            raise ValueError("Invalid damping: {}".format(damping))
+        if not 0.0 < kl_clip:
+            raise ValueError("Invalid clipping value: {}".format(kl_clip))
+        if not 0 < fac_update_freq:
+            raise ValueError("Invalid factor update frequency: {}".format(fac_update_freq))
+        if not 0 < kfac_update_freq:
+            raise ValueError("Invalid K-FAC update frequency: {}".format(kfac_update_freq))
+        if not 0 == kfac_update_freq % fac_update_freq:
+            print("WARNING: it is suggested that kfac_update_freq be a multiple of fac_update_freq")
+        if not 0 < diag_blocks:
+            raise ValueError("Invalid diagonal block approx count: {}".format(diag_blocks))
+        if not 0 <= diag_blocks:
+            raise ValueError("Invalid diagonal block approx count: {}".format(diag_blocks))
+        if not 1 == diag_blocks:
+            print("WARNING: diag_blocks > 1 is experimental and may give poor results.")
+
+        # For compatibility with `KFACParamScheduler`
+        defaults = dict(lr=lr,
+                        damping=damping,
+                        fac_update_freq=fac_update_freq,
+                        kfac_update_freq=kfac_update_freq) 
+
+        super(KFAC, self).__init__(model.parameters(), defaults)
+
+        self.computeA = ComputeA()
+        self.computeG = ComputeG()
+        self.known_modules = {'Linear', 'Conv2d'}
+        self.modules = []
+        self.module_names = []
+        self.fw_factor_handles = []
+        self.bw_factor_handles = []
+        self.module_name_map = {}
+        self._register_modules(model)
+
+        self.steps = 0
+
+        self.fw_merged_comm = MergedCommAllReduce(self.module_names, prefix='forward', merge=False, single_layer=False, symmetric=True, fp16=False)
+        self.bw_merged_comm = MergedCommAllReduce(self.module_names, prefix='backward', merge=False, single_layer=False, symmetric=True, fp16=False)
+        self.multi_comm = MultiTensorComm(symmetric=True, fp16=False)
+
+        # Dictionaries keyed by `module` to storing the factors and
+        # eigendecompositions
+        self.m_a, self.m_g = {}, {}
+        self.m_A, self.m_G = {}, {}
+        self.m_QA, self.m_QG = {}, {}
+        self.m_dA, self.m_dG = {}, {}
+        self.m_dA_ranks = {}
+        self.m_dG_ranks = {}
+        self.module_ranks = None
+
+        self.sparse = sparse
+        self.sparse_ratio = sparse_ratio
+        self.residualsA, self.residualsG = {}, {}
+
+        self.factor_decay = factor_decay
+        self.kl_clip = kl_clip
+        self.fac_update_freq = fac_update_freq
+        self.kfac_update_freq = kfac_update_freq
+        self.diag_blocks = diag_blocks
+        self.diag_warmup = diag_warmup
+        self.batch_averaged = batch_averaged
+
+        self.exclude_communicate_inverse = True if exclude_parts.find('CommunicateInverse') >=0 else False
+        self.exclude_compute_inverse = True if exclude_parts.find('ComputeInverse') >=0 else False
+        self.exclude_communicate_factor = True if exclude_parts.find('CommunicateFactor') >=0 else False
+        self.exclude_compute_factor = True if exclude_parts.find('ComputeFactor') >=0 else False
+        
+        # Compute ideal value for `distribute_layer_factors` based on
+        # registered module count
+        if distribute_layer_factors is None:
+            self.distribute_layer_factors = True \
+                    if hvd.size() > len(self.modules) else False
+        else:
+            self.distribute_layer_factors = distribute_layer_factors
+
+        self.have_cleared_Q = True if self.diag_warmup == 0 else False
+        self.eps = 1e-10  # for numerical stability
+        self.rank_iter = cycle(list(range(hvd.size())))
+
+    def _save_input(self, module, input):
+        """Hook for saving layer input"""
+        if torch.is_grad_enabled() and self.steps % self.fac_update_freq == 0:
+            self.m_a[module] = input[0].data
+
+    def _compute_forward_factor(self, module, input):
+        if torch.is_grad_enabled() and self.steps % self.fac_update_freq == 0:
+            self.m_a[module] = input[0].data
+            self._update_module_A(module)
+
+    def _save_grad_output(self, module, grad_input, grad_output):
+        """Hook for saving gradient w.r.t output"""
+        if self.steps % self.fac_update_freq == 0:
+            self.m_g[module] = grad_output[0].data
+
+    def _compute_backward_factor(self, module, grad_input, grad_output):
+        if self.steps % self.fac_update_freq == 0:
+            self.m_g[module] = grad_output[0].data
+            self._update_module_G(module)
+
+    def _register_modules(self, model):
+        """Register hooks to all supported layers in the model"""
+        name_idx = 0
+        for module in model.modules():
+            classname = module.__class__.__name__
+            if classname in self.known_modules:
+                self.modules.append(module)
+                module.register_forward_pre_hook(self._save_input)
+                module.register_backward_hook(self._save_grad_output)
+                module_name = 'module_name_%s_%d' % (classname, name_idx)
+                self.module_names.append(module_name)
+                self.module_name_map[module] = module_name
+                name_idx += 1
+
+    def _init_A(self, factor, module):
+        """Initialize memory for factor A and its eigendecomp"""
+        self.m_A[module] = torch.diag(factor.new(factor.shape[0]).fill_(1))
+        self.m_dA[module] = factor.new_zeros(factor.shape[0])
+        self.m_QA[module] = factor.new_zeros(factor.shape)
+
+    def _init_G(self, factor, module):
+        """Initialize memory for factor G and its eigendecomp"""
+        self.m_G[module] = torch.diag(factor.new(factor.shape[0]).fill_(1))
+        self.m_dG[module] = factor.new_zeros(factor.shape[0])
+        self.m_QG[module] = factor.new_zeros(factor.shape)
+
+    def _clear_eigen(self):
+        """Clear eigendecompositions
+
+        Useful for when switching between `diag_blocks=1` and `diag-blocks>1`
+        because eigendecompositions saved in place and the off-diagonals must
+        be cleared.
+        """
+        for module in self.modules:
+            self.m_QA[module].fill_(0)
+            self.m_QG[module].fill_(0)
+            self.m_dA[module].fill_(0)
+            self.m_dG[module].fill_(0)
+
+    def _update_module_A(self, module):
+        a = self.computeA(self.m_a[module], module)
+        if self.steps == 0:
+            self._init_A(a, module)
+        update_running_avg(a, self.m_A[module], self.factor_decay)
+        if self.sparse:
+            sparsification(self.m_A[module], module, ratio=self.sparse_ratio, residuals=self.residualsA)
+
+    def _update_A(self):
+        """Compute and update factor A for all modules"""
+        for module in self.modules: 
+            self._update_module_A(module)
+            #if hvd.rank() == 0:
+            #    data = self.m_A[module] #ComputeA.get_data(self.m_a[module], module)
+            #    d = data.view(-1)
+            #    indexes = d.nonzero().data.squeeze().view(-1)
+            #    nnz = indexes.numel() 
+            #    numel = d.numel()
+            #    sparsity = (numel-nnz)*1.0/numel
+            #    logger.info('vector A name: %s, shape: %s, sparsity: %f', module, data.shape, sparsity)
+
+    def _update_module_G(self, module):
+        g = self.computeG(self.m_g[module], module, self.batch_averaged)
+            #logger.info('G Name: %s, shape: %s', module, g.shape)
+        if self.steps == 0:
+            self._init_G(g, module)
+        update_running_avg(g, self.m_G[module], self.factor_decay)
+        if self.sparse:
+            sparsification(self.m_G[module], module, ratio=self.sparse_ratio, residuals=self.residualsG)
+
+    def _update_G(self):
+        """Compute and update factor G for all modules"""
+        for module in self.modules:
+            self._update_module_G(module)
+
+            #if hvd.rank() == 0:
+            #    data = self.m_G[module] #ComputeG.get_data(self.m_g[module], module, self.batch_averaged)
+            #    d = data.view(-1)
+            #    indexes = d.nonzero().data.squeeze().view(-1)
+            #    nnz = indexes.numel() 
+            #    numel = d.numel()
+            #    sparsity = (numel-nnz)*1.0/numel
+            #    logger.info('vector G name: %s, shape: %s, sparsity: %f', module, data.shape, sparsity)
+
+
+    def _update_eigen_A(self, module, ranks):
+        """Compute eigendecomposition of A for module on specified workers
+
+        Note: all ranks will enter this function but only the ranks specified
+        in `ranks` will continue to actually compute the eigendecomposition.
+        All other ranks will simply zero out their buffer for the 
+        eigendecomposition for the current module. This is done so we can sum
+        the eigendecompositions across all ranks to communicate the results
+        of locally computed eigendecompositions.
+
+        Args:
+          module: module to compute eigendecomposition of A on
+          ranks: list of horovod ranks (i.e. workers) to use when computing
+              the eigendecomposition.
+        """
+        if hvd.rank() in ranks:
+            self._distributed_compute_eigen(self.m_A[module], 
+                    self.m_QA[module], self.m_dA[module], ranks)
+        else:
+            self.m_QA[module].fill_(0)
+            self.m_dA[module].fill_(0)
+
+    def _update_eigen_G(self, module, ranks):
+        """Compute eigendecomposition of A for module on specified workers
+
+        See `_update_eigen_A` for more info`
+        """
+        if hvd.rank() in ranks:
+            self._distributed_compute_eigen(self.m_G[module], 
+                    self.m_QG[module], self.m_dG[module], ranks)
+        else:
+            self.m_QG[module].fill_(0)
+            self.m_dG[module].fill_(0)
+
+    def _distributed_compute_eigen(self, factor, evectors, evalues, ranks):
+        """Computes the eigendecomposition of a factor across ranks
+        
+        Assigns each rank in `ranks` to enter this function to compute a
+        diagonal block of `factor`. Results are written to `evectors` and
+        `evalues`. If `len(ranks)==1`, then that rank computes the
+        eigendecomposition of the entire `factor`.
+
+        Args:
+            factor (tensor): tensor to eigendecompose
+            evectors (tensor): tensor to save eigenvectors of `factor` to
+            evalues (tensor): tensor to save eigenvalues of `factor` to
+            ranks (list): list of ranks that will enter this function
+        """
+        i = ranks.index(hvd.rank())
+        n = len(ranks)
+        if n > min(factor.shape):
+            n = min(factor.shape)
+
+        if i < n:
+            start, end = get_block_boundary(i, n, factor.shape)
+            block = factor[start[0]:end[0], start[1]:end[1]]
+            block = add_value_to_diagonal(block, self.damping)
+            inverse = torchsso.utils.inv(block)
+            
+            evectors.data[start[0]:end[0], start[1]:end[1]].copy_(inverse)
+
+    def _get_diag_blocks(self, module, diag_blocks):
+        """Helper method for determining number of diag_blocks to use
+
+        Overrides `diag_blocks` if the `module` does not support
+        `diag_blocks>1`. I.e. for a Linear layer, we do not want to
+        use a `diag_blocks>1`.
+
+        Args:
+          module: module
+          diag_blocks (int): default number of diag blocks to use
+        """
+        return diag_blocks if module.__class__.__name__ == 'Conv2d' else 1
+
+    def _get_grad(self, module):
+        """Get formated gradient of module
+
+        Args:
+          module: module/layer to get gradient of
+
+        Returns:
+          Formatted gradient with shape [output_dim, input_dim] for module
+        """
+        if module.__class__.__name__ == 'Conv2d':
+            # n_filters * (in_c * kw * kh)
+            grad = module.weight.grad.data.view(module.weight.grad.data.size(0), -1)  
+        else:
+            grad = module.weight.grad.data
+        if module.bias is not None:
+            grad = torch.cat([grad, module.bias.grad.data.view(-1, 1)], 1)
+        return grad
+
+    def _get_preconditioned_grad(self, module, grad):
+        """Precondition gradient of module
+        
+        Args:
+          module: module to compute preconditioned gradient for
+          grad: formatted gradient from `_get_grad()`
+
+        Returns:
+          preconditioned gradient with same shape as `grad`
+        """
+        #v = self.m_QG[module].t() @ grad @ self.m_QA[module]
+        v = self.m_QG[module] @ grad @ self.m_QA[module]
+
+        if module.bias is not None:
+            v = [v[:, :-1], v[:, -1:]]
+            v[0] = v[0].view(module.weight.grad.data.size()) # weight
+            v[1] = v[1].view(module.bias.grad.data.size())   # bias
+        else:
+            v = [v.view(module.weight.grad.data.size())]
+        return v
+
+    def _update_scale_grad(self, updates):
+        """Update the gradients in place and scale
+
+        Updates the gradients in-place for all modules using the preconditioned
+        gradients and scales the gradients.
+
+        Args:
+          updates (dict): dict of {module: precon_grad}
+        """
+        vg_sum = 0
+        for module in self.modules:
+            v = updates[module]
+            vg_sum += (v[0] * module.weight.grad.data * self.lr ** 2).sum().item()
+            if module.bias is not None:
+                vg_sum += (v[1] * module.bias.grad.data * self.lr ** 2).sum().item()
+        if self.exclude_communicate_inverse:
+            nu = 1
+        else:
+            nu = min(1.0, math.sqrt(self.kl_clip / abs(vg_sum)))
+
+        for module in self.modules:
+            v = updates[module]
+            module.weight.grad.data.copy_(v[0])
+            module.weight.grad.data.mul_(nu)
+            if module.bias is not None:
+                module.bias.grad.data.copy_(v[1])
+                module.bias.grad.data.mul_(nu)
+
+    def step(self, closure=None, epoch=None):
+        """Perform one K-FAC step
+
+        Note:
+        - this function should always be called before `optimizer.step()`
+        - gradients must be averaged across ranks before calling `step()`
+
+        Args:
+          closure: for compatibility with the base optimizer class.
+              `closure` is ignored by KFAC
+          epoch (int, optional): epoch to use for determining when to end
+              the `diag_warmup` period. `epoch` is not necessary if not using
+              `diag_warmup`
+        """
+
+        # Update params, used for compatibilty with `KFACParamScheduler`
+        group = self.param_groups[0]
+        self.lr = group['lr']
+        self.damping = group['damping']
+        self.fac_update_freq = group['fac_update_freq']
+        self.kfac_update_freq = group['kfac_update_freq']
+
+        updates = {}
+        handles = []
+
+        if epoch is None:
+            if self.diag_warmup > 0:
+                print("WARNING: diag_warmup > 0 but epoch was not passed to "
+                      "KFAC.step(). Defaulting to no diag_warmup")
+            diag_blocks = self.diag_blocks
+        else:
+            diag_blocks = self.diag_blocks if epoch >= self.diag_warmup else 1
+
+        if self.steps % self.fac_update_freq == 0:
+            if not self.exclude_compute_factor:
+                self._update_A()
+                self._update_G()
+            if not self.exclude_communicate_factor:
+                if hvd.size() > 1:
+                    if self.sparse:
+                        self._allgather_factors()
+                    else:
+                        self._allreduce_factors()
+
+        # if we are switching from no diag approx to approx, we need to clear
+        # off-block-diagonal elements
+        if not self.have_cleared_Q and \
+                epoch == self.diag_warmup and \
+                self.steps % self.kfac_update_freq == 0:
+            self._clear_eigen()
+            self.have_cleared_Q = True
+
+        if self.steps % self.kfac_update_freq == 0:
+            # reset rank iter so device get the same layers
+            # to compute to take advantage of caching
+            self.rank_iter.reset() 
+            handles = []
+
+            #eigen_ranks = self._generate_eigen_ranks(epoch)
+            eigen_ranks = self._generate_eigen_ranks_uniform(epoch)
+            #eigen_ranks = self._generate_eigen_ranks_naive(epoch)
+
+            for module in self.modules:
+                ranks_a, ranks_g = eigen_ranks[module]
+                self.m_dA_ranks[module] = ranks_a[0]
+                self.m_dG_ranks[module] = ranks_g[0]
+                rank_a = ranks_a[0]
+                rank_g = ranks_g[0]
+
+                if not self.exclude_compute_inverse:
+                    self._update_eigen_A(module, ranks_a)
+                    self._update_eigen_G(module, ranks_g)
+
+            if not self.exclude_communicate_inverse:
+                if hvd.size() > 1:
+                    self._broadcast_eigendecomp()
+            elif not self.exclude_compute_inverse:
+                # should have a barriar
+                if hvd.size() > 1:
+                    barrier()
+
+        for i, module in enumerate(self.modules):
+            grad = self._get_grad(module)
+            precon_grad = self._get_preconditioned_grad(module, grad)
+            updates[module] = precon_grad
+
+        self._update_scale_grad(updates)
+
+        self.steps += 1
+
+    def _generate_eigen_ranks_naive(self, epoch):
+        if self.module_ranks is not None:
+            return self.module_ranks
+        module_ranks = {}
+        diag_blocks = self.diag_blocks if epoch >= self.diag_warmup else 1
+        buckets = [0] * hvd.size()
+        for module in self.modules:
+            # Get ranks to compute this layer on
+            n = self._get_diag_blocks(module, diag_blocks)
+            ranks_a = self.rank_iter.next(n)
+            ranks_g = self.rank_iter.next(n) if self.distribute_layer_factors \
+                                             else ranks_a
+            module_ranks[module] = (ranks_a, ranks_g)
+            buckets[ranks_a[0]] += self.m_A[module].shape[1]
+            buckets[ranks_g[0]] += self.m_G[module].shape[1]
+        self.module_ranks = module_ranks
+        if hvd.rank() == 0:
+            logger.info('buckets: %s', buckets)
+            logger.info('module_ranks: %s', module_ranks.values())
+        return module_ranks
+
+    def _generate_eigen_ranks_uniform(self, epoch):
+        if self.module_ranks is not None:
+            return self.module_ranks
+        module_ranks = {}
+        diag_blocks = self.diag_blocks if epoch >= self.diag_warmup else 1
+        buckets = [0] * hvd.size()
+        dimensions = []
+        module_factors = []
+        for i, m in enumerate(self.modules):
+            name = self.module_names[i]
+            a_dimension = self.m_A[m].shape[1]
+            g_dimension = self.m_G[m].shape[1]
+            dimensions.append(a_dimension)
+            module_factors.append(name+'-A')
+            dimensions.append(g_dimension)
+            module_factors.append(name+'-G')
+
+        descending_sorted_idx = np.argsort(dimensions)[::-1]
+        A_ranks = {}
+        G_ranks = {}
+        for i in descending_sorted_idx:
+            factor = module_factors[i]
+            dimension = dimensions[i]
+            m_i = self.module_names.index(factor[0:-2])
+            m = self.modules[m_i]
+
+            bi = np.argmin(buckets)
+            buckets[bi] += dimension
+            if factor[-1] == 'A':
+                A_ranks[m] = (bi,)
+            else:
+                G_ranks[m] = (bi,)
+        for m in self.modules:
+            module_ranks[m] = (A_ranks[m], G_ranks[m])
+
+        self.module_ranks = module_ranks
+        if hvd.rank() == 0:
+            logger.info('buckets: %s', buckets)
+            logger.info('module_ranks: %s', module_ranks.values())
+        return module_ranks
+
+    def _generate_eigen_ranks(self, epoch):
+        if self.module_ranks is not None:
+            return self.module_ranks
+        module_ranks = {}
+        diag_blocks = self.diag_blocks if epoch >= self.diag_warmup else 1
+        buckets = [0] * hvd.size()
+
+        for module in self.modules:
+            i = np.argmin(buckets)
+            if hvd.rank() == 0:
+                logger.info('A Name: %s, shape: %s', module, self.m_A[module].shape)
+                logger.info('G Name: %s, shape: %s', module, self.m_G[module].shape)
+            a_dimension = self.m_A[module].shape[1]
+            g_dimension = self.m_G[module].shape[1]
+            #buckets[i] += (a_dimension) + g_dimension)
+            buckets[i] += a_dimension
+            ranks_a = (i,)
+            i = np.argmin(buckets)
+            ranks_g = (i,)
+            buckets[i] += g_dimension
+
+            module_ranks[module] = (ranks_a, ranks_g)
+        self.module_ranks = module_ranks
+        if hvd.rank() == 0:
+            logger.info('buckets: %s', buckets)
+            logger.info('module_ranks: %s', module_ranks.values())
+        return module_ranks
+
+    def _allreduce_factors(self):
+        """Allreduce the factors for all layers"""
+        handles = []
+
+        for m in self.modules:
+            name = self.module_name_map[m]
+            self.fw_merged_comm.allreduce_async_(name, self.m_A[m].data)
+            self.bw_merged_comm.allreduce_async_(name, self.m_G[m].data)
+
+        self.fw_merged_comm.synchronize()
+        self.bw_merged_comm.synchronize()
+
+    def _allgather_factors(self):
+        """Allgather the factors for all layers"""
+        handles = []
+        def _get_value_and_idx(sparse_tensor):
+            tensor = sparse_tensor.data.view(-1)
+            one_indexes = tensor != 0
+            indexes = one_indexes.nonzero().data.squeeze().view(-1)
+            values = tensor.data[indexes] 
+            return values, indexes.int()
+
+        for i, m in enumerate(self.modules):
+            module_name = self.module_names[i]
+
+            A_values, A_indexes = _get_value_and_idx(self.m_A[m].data)
+            A_value_name = module_name + '_A_value'
+            A_idx_name = module_name + '_A_idx'
+            h_value = allgather_async(A_values, A_value_name)
+            h_idx = allgather_async(A_indexes, A_idx_name)
+
+            G_values, G_indexes = _get_value_and_idx(self.m_G[m].data)
+            G_value_name = module_name + '_G_value'
+            G_idx_name = module_name + '_G_idx'
+            h_value_G = allgather_async(G_values, G_value_name)
+            h_idx_G = allgather_async(G_indexes, G_idx_name)
+            handles.append((h_value, h_idx, h_value_G, h_idx_G))
+
+        for i, handle in enumerate(handles):
+            module_name = self.module_names[i]
+            module = self.modules[i]
+            m_A = self.m_A[module].view(-1)
+            m_A.fill_(0.0)
+            m_G = self.m_G[module].view(-1)
+            m_G.fill_(0.0)
+
+            h_value_A, h_idx_A, h_value_G, h_idx_G = handle
+            A_values = hvd.synchronize(h_value_A)
+            A_indexes = hvd.synchronize(h_idx_A).long()
+            m_A.scatter_add_(0, A_indexes, A_values)
+            m_A.div_(hvd.size())
+            
+            G_values = hvd.synchronize(h_value_G)
+            G_indexes = hvd.synchronize(h_idx_G).long()
+            m_G.scatter_add_(0, G_indexes, G_values)
+            m_G.div_(hvd.size())
+
+    def _allreduce_eigendecomp(self):
+        """Allreduce the eigendecompositions for all layers
+
+        Note: we use `op=hvd.Sum` to simulate an allgather`. Each rank will
+        either compute the eigendecomposition for a factor or just return
+        zeros so we sum instead of averaging.
+        """
+        handles = []
+
+        for m in self.modules:
+            handles.append(hvd.allreduce_async_(self.m_QA[m].data, op=hvd.Sum))
+            handles.append(hvd.allreduce_async_(self.m_QG[m].data, op=hvd.Sum))
+            handles.append(hvd.allreduce_async_(self.m_dA[m].data, op=hvd.Sum))
+            handles.append(hvd.allreduce_async_(self.m_dG[m].data, op=hvd.Sum))
+    
+        for handle in handles:
+            hvd.synchronize(handle)
+
+    def _broadcast_eigendecomp(self):
+        """Broadcasts the eigendecompositions for all layers
+
+        Note: we use `op=hvd.Sum` to simulate an allgather`. Each rank will
+        either compute the eigendecomposition for a factor or just return
+        zeros so we sum instead of averaging.
+        """
+        rank = hvd.rank()
+
+        for i, m in enumerate(self.modules):
+            rank_a = self.m_dA_ranks[m]
+            rank_g = self.m_dG_ranks[m]
+            name = self.module_names[i]
+
+            self.multi_comm.bcast_async_([name+'mQA'], [self.m_QA[m]], rank_a)
+            self.multi_comm.bcast_async_([name+'mQG'], [self.m_QG[m]], rank_g)
+        self.multi_comm.synchronize()
+    
+class KFACParamScheduler():
+    """Updates KFAC parameters according to the epoch
+
+    Similar to `torch.optim.lr_scheduler.StepLR()`
+
+    Usage:
+      Call KFACParamScheduler.step() each epoch to compute new parameter
+      values.
+
+    Args:
+      kfac (KFAC): wrapped KFAC preconditioner
+      damping_alpha (float, optional): multiplicative factor of the damping 
+          (default: 1)
+      damping_schedule (list, optional): list of epochs to update the damping
+          by `damping_alpha` (default: None)
+      update_freq_alpha (float, optional): multiplicative factor of the KFAC
+          update freq (default: 1)
+      update_freq_schedule (list, optional): list of epochs to update the KFAC
+          update freq by `update_freq_alpha` (default: None)
+      start_epoch (int, optional): starting epoch, for use if resuming training
+          from checkpoint (default: 0)
+    """
+    def __init__(self,
+                 kfac,
+                 damping_alpha=1,
+                 damping_schedule=None,
+                 update_freq_alpha=1,
+                 update_freq_schedule=None,
+                 start_epoch=0):
+
+        self.kfac = kfac
+        params = self.kfac.param_groups[0]
+
+        self.damping_base = params['damping']
+        self.damping_alpha = damping_alpha
+        self.damping_schedule = damping_schedule
+        self.damping_factor_func = \
+                self._get_factor_func(self.damping_schedule,
+                                     self.damping_alpha)
+
+        self.fac_update_freq_base = params['fac_update_freq']
+        self.kfac_update_freq_base = params['kfac_update_freq']
+        self.update_freq_alpha = update_freq_alpha
+        self.update_freq_schedule = update_freq_schedule
+        self.update_freq_factor_func = \
+                self._get_factor_func(self.update_freq_schedule,
+                                     self.update_freq_alpha)
+
+        self.epoch = start_epoch
+
+    def _get_factor_func(self, schedule, alpha):
+        """Returns a function to compute an update factor using the epoch"""
+        if schedule is not None:
+            schedule.sort(reverse=True)
+        else:
+            schedule = []
+
+        def factor_func(epoch):
+            factor = 1.
+            for e in schedule:
+                if epoch >= e:
+                    factor *= alpha
+            return factor
+
+        return factor_func
+
+    def step(self, epoch=None):
+        """Update KFAC parameters"""
+        if epoch is not None:
+            self.epoch = epoch
+        else:
+            self.epoch += 1
+
+        params = self.kfac.param_groups[0]
+
+        params['damping'] = self.damping_base * self.damping_factor_func(self.epoch)
+
+        factor = self.update_freq_factor_func(self.epoch)
+        params['fac_update_freq'] = int(self.fac_update_freq_base * factor)
+        params['kfac_update_freq'] = int(self.kfac_update_freq_base * factor)
